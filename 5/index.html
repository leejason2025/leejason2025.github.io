<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fun with Diffusion Models</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Raleway:wght@400;700&display=swap');

        body {
            font-family: 'Raleway', sans-serif;
            margin: 2%;
            padding: 0 5%; 
            text-align: center;
            background-color: #E7E9F0;
            color: #051747;
        }

        .title-block {
            background-color: #081F62; 
            color: #E7E9F0;
            padding: 4% 2%; 
            margin-bottom: 3%;
            text-align: center;
        }

        h1 {
            font-size: 2.7em; 
            margin-bottom: 1%;
        }

        h2 {
            font-size: 2.2em;
            margin-bottom: 1%;
        }

        h3 {
            font-size: 1.8em;
        }

        .section-header {
            font-size: 2em;
            margin-top: 4%;
            margin-bottom: 2%;
            color: #081F62;
            text-transform: uppercase;
        }

        section h2 {
            font-size: 1.8em; 
            margin-bottom: 2%;
            margin-top: 5%; 
            position: relative;
        }

        section h2::after {
            content: '';
            display: block;
            width: 10%;
            height: 3px;
            background-color: #081F62; 
            margin: 1% auto; 
        }

        section {
            margin-bottom: 3%;
        }

        .image-item {
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .image-container {
            display: grid;
            gap: 10px;
            justify-items: center;
        }

        img {
            max-width: 100%;
            width: 400px;
            height: auto;
            border: 5px solid #535F80;
        }

        p {
            text-align: left;
            margin-bottom: 2%;
            font-size: 1.1em;
            line-height: 1.6em; 
        }
    </style>
</head>
<body>
    <div class="title-block">
        <h1>Fun with Diffusion Models</h1>
        <h2>Exploring Denoising, Sampling, and Diffusion</h2>
        <h3>Jason Lee</h3>
    </div>

    <section id="intro">
        <h2 class="section-header">Introduction</h2>
        <p>
            Welcome to this journey into diffusion models, where we delve into the fascinating world of generative AI. 
            This project explores key steps like forward processes, denoising, and image-to-image translation while 
            examining the creative potential of these techniques. Through parts A and B, we will cover both theoretical 
            foundations and practical applications.
        </p>
    </section>

    <h2 class="section-header">Part A</h2>

    <section id="setup">
        <h2>0. Set up</h2>
        <p>
            For this project I utilized the DeepFloyd IF diffusion model, a two-stage generative model trained by Stability AI. 
            The first stage generates base images, and the second refines them into higher-resolution outputs. To start, here are some of the
images generated using these prompts: 'an oil painting of a snowy mountain village', 'a man wearing a hat', and 'a rocket ship'. They are tested at different number of steps. When running with differing number of steps, it seems that when the step size increases, the images get more detailed. That doesn't mean that it is more realistic, just that there are more components and forms. I set the seed to 88.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/20hat.png" alt="Setup Image 1"><p>steps = 20</p></div>
            <div class="image-item"><img src="media/50hat.png" alt="Setup Image 2"><p>steps = 20</p></div>
            <div class="image-item"><img src="media/10rocket.png" alt="Setup Image 3"><p>steps = 10</p></div>
            <div class="image-item"><img src="media/50rocket.png" alt="Setup Image 4"><p>steps = 50</p></div>
            <div class="image-item"><img src="media/10snow.png" alt="Setup Image 5"><p>steps = 10</p></div>
            <div class="image-item"><img src="media/20snow.png" alt="Setup Image 6"><p>steps = 20</p></div>
        </div>
    </section>

    <section id="forward-process">
        <h2>1.1 Forward Process</h2>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/forward1.png" alt="Forward Process Image 1"><p>Forward 1</p></div>
            <div class="image-item"><img src="media/forward2.png" alt="Forward Process Image 2"><p>Forward 2</p></div>
            <div class="image-item"><img src="media/forward3.png" alt="Forward Process Image 3"><p>Forward 3</p></div>
        </div>
    </section>

    <section id="classical-denoising">
        <h2>1.2 Classical Denoising</h2>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/denoise1.png" alt="Denoising 1"><p>Denoising 1</p></div>
            <div class="image-item"><img src="media/denoise2.png" alt="Denoising 2"><p>Denoising 2</p></div>
            <div class="image-item"><img src="media/denoise3.png" alt="Denoising 3"><p>Denoising 3</p></div>
            <div class="image-item"><img src="media/denoise4.png" alt="Denoising 4"><p>Denoising 4</p></div>
            <div class="image-item"><img src="media/denoise5.png" alt="Denoising 5"><p>Denoising 5</p></div>
            <div class="image-item"><img src="media/denoise6.png" alt="Denoising 6"><p>Denoising 6</p></div>
        </div>
    </section>

    <section id="one-step-denoising">
        <h2>1.3 One Step Denoising</h2>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/onestep1.png" alt="One Step 1"><p>One Step 1</p></div>
            <div class="image-item"><img src="media/onestep2.png" alt="One Step 2"><p>One Step 2</p></div>
            <div class="image-item"><img src="media/onestep3.png" alt="One Step 3"><p>One Step 3</p></div>
            <div class="image-item"><img src="media/onestep4.png" alt="One Step 4"><p>One Step 4</p></div>
            <div class="image-item"><img src="media/onestep5.png" alt="One Step 5"><p>One Step 5</p></div>
            <div class="image-item"><img src="media/onestep6.png" alt="One Step 6"><p>One Step 6</p></div>
            <div class="image-item"><img src="media/onestep7.png" alt="One Step 7"><p>One Step 7</p></div>
            <div class="image-item"><img src="media/onestep8.png" alt="One Step 8"><p>One Step 8</p></div>
            <div class="image-item"><img src="media/onestep9.png" alt="One Step 9"><p>One Step 9</p></div>
        </div>
    </section>

    <section id="iterative-denoising">
        <h2>1.4 Iterative Denoising</h2>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/iterative1.png" alt="Iterative 1"><p>Iterative 1</p></div>
            <div class="image-item"><img src="media/iterative2.png" alt="Iterative 2"><p>Iterative 2</p></div>
            <div class="image-item"><img src="media/iterative3.png" alt="Iterative 3"><p>Iterative 3</p></div>
        </div>
    </section>

    <section id="diffusion-model-sampling">
        <h2>1.5 Diffusion Model Sampling</h2>
        <p>
            Sampling from a trained diffusion model is a critical step in generating meaningful outputs. 
            This section demonstrates the results of sampling, showcasing how the model reconstructs high-quality images 
            from noise step by step.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(5, 1fr);">
            <div class="image-item"><img src="media/sample1.png" alt="Sample 1"><p>Sample 1</p></div>
            <div class="image-item"><img src="media/sample2.png" alt="Sample 2"><p>Sample 2</p></div>
            <div class="image-item"><img src="media/sample3.png" alt="Sample 3"><p>Sample 3</p></div>
            <div class="image-item"><img src="media/sample4.png" alt="Sample 4"><p>Sample 4</p></div>
            <div class="image-item"><img src="media/sample5.png" alt="Sample 5"><p>Sample 5</p></div>
        </div>
    </section>

    <section id="classifier-free-guidance">
        <h2>1.6 Classifier Free Guidance</h2>
        <p>
            Classifier-free guidance enhances generation by steering the diffusion process towards more desired outputs 
            using conditioning information. Here are examples of results with different levels of guidance applied.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(5, 1fr);">
            <div class="image-item"><img src="media/guidance1.png" alt="Guidance 1"><p>Guidance 1</p></div>
            <div class="image-item"><img src="media/guidance2.png" alt="Guidance 2"><p>Guidance 2</p></div>
            <div class="image-item"><img src="media/guidance3.png" alt="Guidance 3"><p>Guidance 3</p></div>
            <div class="image-item"><img src="media/guidance4.png" alt="Guidance 4"><p>Guidance 4</p></div>
            <div class="image-item"><img src="media/guidance5.png" alt="Guidance 5"><p>Guidance 5</p></div>
        </div>
    </section>

    <section id="image-to-image-translation">
        <h2>1.7 Image to Image Translation</h2>
        <p>
            Image-to-image translation involves transforming one image into another while preserving its content 
            but applying stylistic or contextual changes. Below are three subcategories showcasing various applications.
        </p>

        <section id="editing-hand-drawn-web">
            <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
            <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
                <!-- 21 images (3 rows of 7) -->
                <div class="image-item"><img src="media/hand_drawn1.png" alt="Hand-Drawn 1"></div>
                <div class="image-item"><img src="media/hand_drawn2.png" alt="Hand-Drawn 2"></div>
                <div class="image-item"><img src="media/hand_drawn3.png" alt="Hand-Drawn 3"></div>
                <!-- Repeat for 21 images -->
            </div>
        </section>

        <section id="inpainting">
            <h3>1.7.2 Inpainting</h3>
            <div class="image-container" style="grid-template-columns: repeat(4, 1fr);">
                <!-- 12 images (3 rows of 4) -->
                <div class="image-item"><img src="media/inpaint1.png" alt="Inpaint 1"></div>
                <div class="image-item"><img src="media/inpaint2.png" alt="Inpaint 2"></div>
                <div class="image-item"><img src="media/inpaint3.png" alt="Inpaint 3"></div>
                <!-- Repeat for 12 images -->
            </div>
        </section>

        <section id="text-conditioned-image-to-image">
            <h3>1.7.3 Text-Conditioned Image-to-Image Translation</h3>
            <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
                <!-- 21 images (3 rows of 7) -->
                <div class="image-item"><img src="media/text_image1.png" alt="Text Conditioned 1"></div>
                <div class="image-item"><img src="media/text_image2.png" alt="Text Conditioned 2"></div>
                <div class="image-item"><img src="media/text_image3.png" alt="Text Conditioned 3"></div>
                <!-- Repeat for 21 images -->
            </div>
        </section>
    </section>

    <section id="visual-anagrams">
        <h2>1.8 Visual Anagrams</h2>
        <p>
            Visual anagrams involve creatively rearranging visual elements to explore new patterns and forms. 
            Below are examples of anagrams generated using diffusion models.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/anagram1.png" alt="Anagram 1"></div>
            <div class="image-item"><img src="media/anagram2.png" alt="Anagram 2"></div>
            <div class="image-item"><img src="media/anagram3.png" alt="Anagram 3"></div>
        </div>
    </section>

    <section id="hybrid-images">
        <h2>1.10 Hybrid Images</h2>
        <p>
            Hybrid images blend features from multiple source images to create a unique visual output. These examples highlight 
            the fusion of textures, colors, and patterns using diffusion techniques.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/hybrid1.png" alt="Hybrid 1"></div>
            <div class="image-item"><img src="media/hybrid2.png" alt="Hybrid 2"></div>
            <div class="image-item"><img src="media/hybrid3.png" alt="Hybrid 3"></div>
        </div>
    </section>

    <h2 class="section-header">Part B</h2>

    <section id="training-single-step">
        <h2>Part 1: Training a Single-Step Denoising UNet</h2>
        <p>
            Explore the steps of training a UNet for single-step denoising and its capabilities in handling 
            complex image distributions and generalizing to unseen data.
        </p>
    </section>

    <section id="implementing-unet">
        <h3>1.1 Implementing the UNet</h3>
        <p>
            The UNet architecture is a versatile tool for denoising. Here, we implement and experiment with its structure.
        </p>
    </section>

    <section id="using-unet">
        <h3>1.2 Using the UNet to Train a Denoiser</h3>
        <p>
            By training the UNet with noisy image inputs and clean outputs, the model learns to reconstruct images 
            effectively.
        </p>
        <h4>1.2.1 Training</h4>
        <p>Details of the training process go here...</p>
        <h4>1.2.2 Out-of-Distribution Testing</h4>
        <p>Testing the trained model on unseen data for robustness.</p>
    </section>

    <section id="training-diffusion-model">
        <h2>Part 2: Training a Diffusion Model</h2>
        <h3>2.1 Adding Time Conditioning to UNet</h3>
        <p>Time-conditioning adapts the UNet for step-wise sampling in diffusion models.</p>
        <h3>2.2 Training the UNet</h3>
        <p>Results from training the UNet for diffusion tasks.</p>
        <h3>2.3 Sampling from the UNet</h3>
        <p>Sampling high-quality images from a trained UNet.</p>
        <h3>2.4 Adding Class-Conditioning to UNet</h3>
        <p>Introducing class-conditioning for targeted generation.</p>
        <h3>2.5 Sampling from the Class-Conditioned UNet</h3>
        <p>Examples of class-conditioned outputs generated by the UNet.</p>
    </section>
</body>
</html>
