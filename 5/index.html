<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fun with Diffusion Models</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Raleway:wght@400;700&display=swap');

        body {
            font-family: 'Raleway', sans-serif;
            margin: 2%;
            padding: 0 5%; 
            text-align: center;
            background-color: #E7E9F0;
            color: #051747;
        }

        .title-block {
            background-color: #081F62; 
            color: #E7E9F0;
            padding: 4% 2%; 
            margin-bottom: 3%;
            text-align: center;
        }

        h1 {
            font-size: 2.7em; 
            margin-bottom: 1%;
        }

        h2 {
            font-size: 2.2em;
            margin-bottom: 1%;
        }

        h3 {
            font-size: 1.8em;
        }

        .section-header {
            font-size: 2em;
            margin-top: 4%;
            margin-bottom: 2%;
            color: #081F62;
            text-transform: uppercase;
        }

        section h2 {
            font-size: 1.8em; 
            margin-bottom: 2%;
            margin-top: 5%; 
            position: relative;
        }

        section h2::after {
            content: '';
            display: block;
            width: 10%;
            height: 3px;
            background-color: #081F62; 
            margin: 1% auto; 
        }

        section {
            margin-bottom: 3%;
        }

        .image-item {
            text-align: center;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .image-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); /* Adjust the minimum width */
            gap: 10px;
        }


        img {
            max-width: 100%;
            width: 400px;
            height: auto;
            border: 5px solid #535F80;
        }

        p {
            text-align: left;
            margin-bottom: 2%;
            font-size: 1.1em;
            line-height: 1.6em; 
        }
    </style>
</head>
<body>
    <div class="title-block">
        <h1>Fun with Diffusion Models</h1>
        <h2>Exploring Denoising, Sampling, and Diffusion</h2>
        <h3>Jason Lee</h3>
    </div>

    <section id="intro">
        <h2 class="section-header">Introduction</h2>
        <p>
            Welcome to this journey into diffusion models, where we delve into the fascinating world of generative AI. 
            This project explores key steps like forward processes, denoising, and image-to-image translation while 
            examining the creative potential of these techniques. Through parts A and B, we will cover both theoretical 
            foundations and practical applications.
        </p>
    </section>

    <h2 class="section-header">Part A</h2>

    <section id="setup">
        <h2>0. Set up</h2>
        <p>
            For this project I utilized the DeepFloyd IF diffusion model, a two-stage generative model trained by Stability AI. 
            The first stage generates base images, and the second refines them into higher-resolution outputs. To start, here are some of the
images generated using these prompts: 'an oil painting of a snowy mountain village', 'a man wearing a hat', and 'a rocket ship'. They are tested at different number of steps. When running with differing number of steps, it seems that when the step size increases, the images get more detailed. That doesn't mean that it is more realistic, just that there are more components and forms. I set the seed to 88.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/20hat.png" alt="Setup Image 1"><p>steps = 20</p></div>
            <div class="image-item"><img src="media/10rocket.png" alt="Setup Image 2"><p>steps = 10</p></div>
            <div class="image-item"><img src="media/10snow.png" alt="Setup Image 3"><p>steps = 10</p></div>
            <div class="image-item"><img src="media/50hat.png" alt="Setup Image 4"><p>steps = 50</p></div>
            <div class="image-item"><img src="media/50rocket.png" alt="Setup Image 5"><p>steps = 50</p></div>
            <div class="image-item"><img src="media/20snow.png" alt="Setup Image 6"><p>steps = 20</p></div>
        </div>
    </section>

    <section id="forward-process">
        <h2>1.1 Forward Process</h2>
        <p>
            The forward process adds noise to clean images progressively, turning them into pure noise. This process is reversed during sampling. Using a predefined formula, I add noise to a test image at different levels and observe the gradual degradation into noise.

        </p>
        <div class="image-container" style="grid-template-columns: repeat(4, 1fr);">
            <div class="image-item"><img src="media/orig.png" alt="Forward Process Image 1"><p>Original</p></div>
            <div class="image-item"><img src="media/n250.png" alt="Forward Process Image 1"><p>noise = 250</p></div>
            <div class="image-item"><img src="media/n500.png" alt="Forward Process Image 2"><p>noise = 500</p></div>
            <div class="image-item"><img src="media/n750.png" alt="Forward Process Image 3"><p>noise = 750</p></div>
        </div>
    </section>

    <section id="classical-denoising">
        <h2>1.2 Classical Denoising</h2>
        <p>
            
I attempted to restore noisy images using classical Gaussian blur techniques. This method struggles to produce high-quality results, revealing the limitations of traditional approaches compared to modern diffusion methods.

        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/n250.png" alt="Forward Process Image 1"><p>noise = 250</p></div>
            <div class="image-item"><img src="media/n500.png" alt="Forward Process Image 2"><p>noise = 500</p></div>
            <div class="image-item"><img src="media/n750.png" alt="Forward Process Image 3"><p>noise = 750</p></div>
            
            <div class="image-item"><img src="media/g250.png" alt="Forward Process Image 1"><p>250 denoised</p></div>
            <div class="image-item"><img src="media/g500.png" alt="Forward Process Image 2"><p>500 denoised</p></div>
            <div class="image-item"><img src="media/g750.png" alt="Forward Process Image 3"><p>750 denoised</p></div>
        </div>
    </section>

    <section id="one-step-denoising">
        <h2>1.3 One Step Denoising</h2>
        <p>
With the pretrained DeepFloyd UNet, I estimated and removed noise from noisy images at various levels.
            This highlights the model's ability to recover high-quality images by projecting noisy inputs back to the image manifold. However, one step is not sufficient enough to produce crystal clear results.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/o250.png" alt="One Step 1"><p>One Step 250</p></div>
            <div class="image-item"><img src="media/o500.png" alt="One Step 2"><p>One Step 500</p></div>
            <div class="image-item"><img src="media/0750.png" alt="One Step 3"><p>One Step 750</p></div>
        </div>
    </section>

    <section id="iterative-denoising">
        <h2>1.4 Iterative Denoising</h2>
        <p>
            Diffusion models excel at iterative denoising. By denoising in steps, I improved the quality
            of noisy images incrementally. Striding through timesteps optimizes this process, 
            making a balance between efficiency and accuracy.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(5, 1fr);">
            <div class="image-item"><img src="media/i0.png" alt="Sample 1"><p>Step 0</p></div>
            <div class="image-item"><img src="media/i5.png" alt="Sample 2"><p>Step 5/p></div>
            <div class="image-item"><img src="media/i10.png" alt="Sample 3"><p>Step 10</p></div>
            <div class="image-item"><img src="media/i15.png" alt="Sample 4"><p>Step 15</p></div>
            <div class="image-item"><img src="media/i20.png" alt="Sample 5"><p>Step 20</p></div>
        </div>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/i.png" alt="Iterative 1"><p>Iterative Denoise</p></div>
            <div class="image-item"><img src="media/io.png" alt="Iterative 2"><p>One Step Denoise</p></div>
            <div class="image-item"><img src="media/ig.png" alt="Iterative 3"><p>Guassian Denoise</p></div>
        </div>
    </section>

    <section id="diffusion-model-sampling">
        <h2>1.5 Diffusion Model Sampling</h2>
        <p>
            Starting with pure noise, I used the iterative denoising process to generate images from scratch. 
            This showcases the model's creative potential and ability to synthesize realistic visuals. Here are 5 samples I generated.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(5, 1fr);">
            <div class="image-item"><img src="media/s1.png" alt="Sample 1"><p>Sample 1</p></div>
            <div class="image-item"><img src="media/s2.png" alt="Sample 2"><p>Sample 2</p></div>
            <div class="image-item"><img src="media/s3.png" alt="Sample 3"><p>Sample 3</p></div>
            <div class="image-item"><img src="media/s4.png" alt="Sample 4"><p>Sample 4</p></div>
            <div class="image-item"><img src="media/s5.png" alt="Sample 5"><p>Sample 5</p></div>
        </div>
    </section>

    <section id="classifier-free-guidance">
        <h2>1.6 Classifier Free Guidance</h2>
        <p>
            Classifier-free guidance enhances the images made by steering the diffusion process towards more desired outputs 
            using conditioning information. Here are 5 samples I generated.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(5, 1fr);">
            <div class="image-item"><img src="media/g1.png" alt="Guidance 1"><p>Guidance Sample 1</p></div>
            <div class="image-item"><img src="media/g2.png" alt="Guidance 2"><p>Guidance Sample 2</p></div>
            <div class="image-item"><img src="media/g3.png" alt="Guidance 3"><p>Guidance Sample 3</p></div>
            <div class="image-item"><img src="media/g4.png" alt="Guidance 4"><p>Guidance Sample 4</p></div>
            <div class="image-item"><img src="media/g5.png" alt="Guidance 5"><p>Guidance Sample 5</p></div>
        </div>
    </section>

    <section id="image-to-image-translation">
        <h2>1.7 Image to Image Translation</h2>
        <p>
            Image-to-image translation involves transforming one image into another while preserving its content 
            but applying stylistic or contextual changes. Here are the transformations using the prompt "a high quality photo"
            to the test image of the campinile and to two images of charachters from a show called Arcane
        </p>
        <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
            <div class="image-item"><img src="media/txt1.png" alt="Sample 1"><p>noise = 1</p></div>
            <div class="image-item"><img src="media/txt2.png" alt="Sample 2"><p>noise = 3</p></div>
            <div class="image-item"><img src="media/txt3.png" alt="Sample 3"><p>noise = 5</p></div>
            <div class="image-item"><img src="media/txt4.png" alt="Sample 4"><p>noise = 7</p></div>
            <div class="image-item"><img src="media/txt5.png" alt="Sample 5"><p>noise = 10</p></div>
            
            <div class="image-item"><img src="media/txt6.png" alt="Sample 6"><p>noise = 20</p></div>
            
            <div class="image-item"><img src="media/orig.png" alt="Sample 5"><p>original</p></div>
        </div>
        <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
            <div class="image-item"><img src="media/A1.png" alt="Sample 1"><p>noise = 1</p></div>
            <div class="image-item"><img src="media/A2.png" alt="Sample 2"><p>noise = 3</p></div>
            <div class="image-item"><img src="media/A3.png" alt="Sample 3"><p>noise = 5</p></div>
            <div class="image-item"><img src="media/A4.png" alt="Sample 4"><p>noise = 7</p></div>
            <div class="image-item"><img src="media/A5.png" alt="Sample 5"><p>noise = 10</p></div>
            
            <div class="image-item"><img src="media/A6.png" alt="Sample 6"><p>noise = 20</p></div>
            
            <div class="image-item"><img src="media/origA.png" alt="Sample 5"><p>original</p></div>
        </div>
        <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
            <div class="image-item"><img src="media/B1.png" alt="Sample 1"><p>noise = 1</p></div>
            <div class="image-item"><img src="media/B2.png" alt="Sample 2"><p>noise = 3</p></div>
            <div class="image-item"><img src="media/B3.png" alt="Sample 3"><p>noise = 5</p></div>
            <div class="image-item"><img src="media/B4.png" alt="Sample 4"><p>noise = 7</p></div>
            <div class="image-item"><img src="media/B5.png" alt="Sample 5"><p>noise = 10</p></div>
            
            <div class="image-item"><img src="media/B6.png" alt="Sample 6"><p>noise = 20</p></div>
            
            <div class="image-item"><img src="media/origB.png" alt="Sample 5"><p>original</p></div>
        </div>

        <section id="editing-hand-drawn-web">
            <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
            <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
                <!-- 21 images (3 rows of 7) -->
                <div class="image-item"><img src="media/hand_drawn1.png" alt="Hand-Drawn 1"></div>
                <div class="image-item"><img src="media/hand_drawn2.png" alt="Hand-Drawn 2"></div>
                <div class="image-item"><img src="media/hand_drawn3.png" alt="Hand-Drawn 3"></div>
                <!-- Repeat for 21 images -->
            </div>
        </section>

        <section id="inpainting">
            <h3>1.7.2 Inpainting</h3>
            <div class="image-container" style="grid-template-columns: repeat(4, 1fr);">
                <!-- 12 images (3 rows of 4) -->
                <div class="image-item"><img src="media/inpaint1.png" alt="Inpaint 1"></div>
                <div class="image-item"><img src="media/inpaint2.png" alt="Inpaint 2"></div>
                <div class="image-item"><img src="media/inpaint3.png" alt="Inpaint 3"></div>
                <!-- Repeat for 12 images -->
            </div>
        </section>

        <section id="text-conditioned-image-to-image">
            <h3>1.7.3 Text-Conditioned Image-to-Image Translation</h3>
            <div class="image-container" style="grid-template-columns: repeat(7, 1fr);">
                <!-- 21 images (3 rows of 7) -->
                <div class="image-item"><img src="media/text_image1.png" alt="Text Conditioned 1"></div>
                <div class="image-item"><img src="media/text_image2.png" alt="Text Conditioned 2"></div>
                <div class="image-item"><img src="media/text_image3.png" alt="Text Conditioned 3"></div>
                <!-- Repeat for 21 images -->
            </div>
        </section>
    </section>

    <section id="visual-anagrams">
        <h2>1.8 Visual Anagrams</h2>
        <p>
            Visual anagrams involve creatively rearranging visual elements to explore new patterns and forms. 
            Below are examples of anagrams generated using diffusion models.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/anagram1.png" alt="Anagram 1"></div>
            <div class="image-item"><img src="media/anagram2.png" alt="Anagram 2"></div>
            <div class="image-item"><img src="media/anagram3.png" alt="Anagram 3"></div>
        </div>
    </section>

    <section id="hybrid-images">
        <h2>1.10 Hybrid Images</h2>
        <p>
            Hybrid images blend features from multiple source images to create a unique visual output. These examples highlight 
            the fusion of textures, colors, and patterns using diffusion techniques.
        </p>
        <div class="image-container" style="grid-template-columns: repeat(3, 1fr);">
            <div class="image-item"><img src="media/hybrid1.png" alt="Hybrid 1"></div>
            <div class="image-item"><img src="media/hybrid2.png" alt="Hybrid 2"></div>
            <div class="image-item"><img src="media/hybrid3.png" alt="Hybrid 3"></div>
        </div>
    </section>

    <h2 class="section-header">Part B</h2>

    <section id="training-single-step">
        <h2>Part 1: Training a Single-Step Denoising UNet</h2>
        <p>
            Explore the steps of training a UNet for single-step denoising and its capabilities in handling 
            complex image distributions and generalizing to unseen data.
        </p>
    </section>

    <section id="implementing-unet">
        <h3>1.1 Implementing the UNet</h3>
        <p>
            The UNet architecture is a versatile tool for denoising. Here, we implement and experiment with its structure.
        </p>
    </section>

    <section id="using-unet">
        <h3>1.2 Using the UNet to Train a Denoiser</h3>
        <p>
            By training the UNet with noisy image inputs and clean outputs, the model learns to reconstruct images 
            effectively.
        </p>
        <h4>1.2.1 Training</h4>
        <p>Details of the training process go here...</p>
        <h4>1.2.2 Out-of-Distribution Testing</h4>
        <p>Testing the trained model on unseen data for robustness.</p>
    </section>

    <section id="training-diffusion-model">
        <h2>Part 2: Training a Diffusion Model</h2>
        <h3>2.1 Adding Time Conditioning to UNet</h3>
        <p>Time-conditioning adapts the UNet for step-wise sampling in diffusion models.</p>
        <h3>2.2 Training the UNet</h3>
        <p>Results from training the UNet for diffusion tasks.</p>
        <h3>2.3 Sampling from the UNet</h3>
        <p>Sampling high-quality images from a trained UNet.</p>
        <h3>2.4 Adding Class-Conditioning to UNet</h3>
        <p>Introducing class-conditioning for targeted generation.</p>
        <h3>2.5 Sampling from the Class-Conditioned UNet</h3>
        <p>Examples of class-conditioned outputs generated by the UNet.</p>
    </section>
</body>
</html>
